[{"authors":["admin"],"categories":null,"content":"I am currently a doctoral candidate at the Vienna Graduate School of Finance and will join the Department of Economics at the University in Copenhagen as a tenure-track assistant professor in finance from August 2020.\nI have a deep interest in the economic implications and evolution of blockchain-based settlement in financial markets. I pursue research questions related to market fragmentation, high frequency trading and big data in financial applications. My research is thus anchored in the intersection of market microstructure, asset pricing and financial econometrics.\nYou can find my current research on SSRN and arXiv.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1574606542,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://voigtstefan.me/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently a doctoral candidate at the Vienna Graduate School of Finance and will join the Department of Economics at the University in Copenhagen as a tenure-track assistant professor in finance from August 2020.","tags":null,"title":"Stefan Voigt","type":"authors"},{"authors":["Stefan Voigt"],"categories":[],"content":"\rAfter the warm-up in the last blog, I’ll move to a more serious analysis of data from Lobster.\n\rShina App Iframe\r\r\rDownloading and extracting massive data\rFirst, I requested all orderbook messages from Lobster up to level 10 from June 2012 until March 2020. During that period, SPY trading was very active: I observe more than 4.26 billion messages. Total trading volume of SPY on NASDAQ during that period exceeded 5.35 Trillion USD.\nLobster compiles the data on request and provides downloadable .7z files after processing the messages. To download everything (on a Linux machine), it is advisable to make use of wget (you’ll have to replace username, password and user_id with your own credentials):\nwget -bqc -P lobster_raw ftp://username:password@lobsterdata.com/user_id/*\rAs a next step, extract the .7z files before working with the individual files - although it is possible to read in the files from within the zipped folder, I made the experience that this can cause problems when done in parallel.\n7z e .lobster_raw/SPY_2019-06-27_2020-03-26_10.7z -o./data/lobster\r7z e .lobster_raw/SPY_2018-06-27_2019-06-26_10.7z -o./data/lobster\r7z e .lobster_raw/SPY_2017-06-27_2018-06-26_10.7z -o./data/lobster\r....\r1698 trading days occupy roughly 2 Terabyte of hard drive space. As explained in my previous post, I compute summary statistics for each single day in my sample. For the sake of brevity, the code snippet below is everything needed to do this in a straightforward parallel fashion using Slurm Workload Manager (the actual task 01_summarise_lobster_messages.R can be downloaded here).\n#$ -N lobster_summary\r#$ -t 1:1698\r#$ -e SPY_Investigation/Chunk\r#$ -o SPY_Investigation/Chunk\rR-g --vanilla \u0026lt; SPY_Investigation/01_summarise_lobster_messages.R\r\rMerge and summarise\rNext, I merge and evaluate the resulting files.\n# Required packages\rlibrary(tidyverse)\rlibrary(lubridate)\r# Asset and Date information\rasset \u0026lt;- \u0026quot;SPY\u0026quot;\rexisting_files \u0026lt;- dir(pattern=paste0(\u0026quot;LOBSTER_\u0026quot;, asset, \u0026quot;.*_summary.csv\u0026quot;), path=\u0026quot;output/summary_files\u0026quot;,\rfull.names = TRUE)\rsummary_data \u0026lt;- map(existing_files, function(x)\r{read_csv(x, col_names = TRUE, cols(ts_minute = col_datetime(format = \u0026quot;\u0026quot;),\rmidquote = col_double(),\rspread = col_double(),\rvolume = col_double(),\rhidden_volume = col_double(),\rdepth_bid = col_double(),\rdepth_ask = col_double(),\rdepth_bid_5 = col_double(),\rdepth_ask_5 = col_double(),\rmessages = col_double()))})\rsummary_data \u0026lt;- summary_data %\u0026gt;% bind_rows()\rwrite_csv(summary_data, paste0(\u0026quot;output/LOBSTER_\u0026quot;,asset,\u0026quot;_summary.csv\u0026quot;))\r\rSPY Depth is at an all-time low\rIn their paper Bid Price Dispersion, Albert Menkveld and Boyan Jovanovic document (among many other interesting things) a striking downwards trend in depth of the orderbook of SPY, the most actively traded ETF in the world.\ndata_by_date \u0026lt;- data %\u0026gt;% mutate (date = ymd(floor_date(ts_minute, \u0026quot;day\u0026quot;))) %\u0026gt;%\rgroup_by(date) %\u0026gt;% select(-ts_minute) %\u0026gt;% summarise_all(median)\rFeel free to play around with the Shiny Gadget at the beginning of the post to convince yourself: We see a negative trend in quoted spreads (apart from a couple of outliers) but as the figure below simultaneously shows, quoted depth at the best level as well as 5 basis points apart from the concurrent midquote decreased as well - note the extreme drop in liquidity provisioning since the beginning of 2020. The red line in the figure shows the daily average number of shares at the best ask (blue line corresponds to the bid). The dashed lines correspond to depth at 5 basis points (the number of shares available within 5 basis points from the midquote). Note that the y-axis is in a log scale, thus the figure hints at much more mass of the depth around the best levels.\r\rCOVID19 and the SP500\rNeedless to say, COVID19 caused turbulent days for global financial markets. The figure below illustrates how quoted liquidity and trading activity changed since January 13th, 2020, the first day WHO reported a case outside of China. More specifically, I plot the intra-daily dynamics of some of the derived measures for the entire year 2019 and the last couple of weeks.\ncorona_threshold \u0026lt;- \u0026quot;2020-01-13\u0026quot;\rbin_data \u0026lt;- data %\u0026gt;% mutate(\rbin = ymd_hms(cut(ts_minute, \u0026quot;5 min\u0026quot;)),\rbin = strftime(bin, format=\u0026quot;%H:%M:%S\u0026quot;),\rbin = as.POSIXct(bin, format=\u0026quot;%H:%M:%S\u0026quot;)) %\u0026gt;%\rselect(bin, everything()) %\u0026gt;% filter(ts_minute \u0026gt; \u0026quot;01-01-2019\u0026quot;,\r(hour(bin)\u0026gt;\u0026quot;09\u0026quot; \u0026amp; minute(bin)\u0026gt;\u0026quot;35\u0026quot;) | (hour(bin)\u0026lt;=\u0026quot;15\u0026quot; \u0026amp; minute(bin)\u0026lt;\u0026quot;55\u0026quot;)) %\u0026gt;% group_by(bin, Corona = ts_minute\u0026gt;=corona_threshold) %\u0026gt;% summarise_all(list(mean=mean)) \r\rShina App Iframe\r\r\r\r","date":1585267200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585336043,"objectID":"34e1191a55c3dda81dd415398723cf3b","permalink":"https://voigtstefan.me/lobster-large-scale-liquidity-analysis/","publishdate":"2020-03-27T00:00:00Z","relpermalink":"/lobster-large-scale-liquidity-analysis/","section":"","summary":"A short  series of posts on handling high-frequency data from Lobster and R","tags":[],"title":"LobsteR - Analysing a Decade of High-Frequency Trading","type":"page"},{"authors":["Stefan Voigt"],"categories":[],"content":"\rDuring my PhD studies, I have been working with high-frequency trading data provided by Lobster a lot for some of my research projects.\nIn this short series of posts, I want share some of my code and routines to efficiently handly the extremely large amounts of data that go through NASDAQs servers on a daily basis. In fact, if you look at the figure below, there is plenty to explore: during less than 2 minutes on March 17th, 2020, thousands of trades have been executed for SPY, a large ETF. The red line shows the traded prices during that period and the blue shaded areas show the dynamics of the orderbook. The darker the areas, the more liquidity (measured as size of the order book levels).\nFirst, I provide some snippets to read-in Lobster files and to compute some potentially interesting statistics. In a second post, I illustrate long-run characteristics of the orderbook dynamics and I’ll finally focus some really recent events: the days since the outbreak of COVID19 have been extremely bumpy for SPY, the largest ETF in the world and it is amazing to see, how liquidity supply changed during these rough days.\nHandling Lobster Data\rLobster is an online limit order book data tool to provide easy-to-use, high-quality limit order book data for the entire universe of NASDAQ traded stocks. I requested some of the data based on their online interface and stored it before running the code below.\rThe actual data which I will use for the next post is much larger. I downloaded all trading messages for ticker SPY (order submissions, cancellations, trades, …) that went through NASDAQ since July, 27th 2007 until March, 25th, 2020. The files contain the entire orderbooks until level 10.\nFirst steps\rI work in R with message level data from Lobster in a tidy and (hopefully) efficient way.\nlibrary(tidyverse)\rlibrary(lubridate)\rAs an example, I illustrate the computations for a tiny glimpse of March 17th, 2020. Lobster files always come with the same naming convention ticker_date_34200000_57600000_filetype_level.csv, whereas filetype either denotes message or the corresponding orderbook snapshots.\nasset \u0026lt;- \u0026quot;SPY\u0026quot;\rdate \u0026lt;- \u0026quot;2020-03-17\u0026quot;\rlevel \u0026lt;- 10\rmessages_filename \u0026lt;- paste0(asset,\u0026quot;_\u0026quot;,date,\u0026quot;_34200000_57600000_message_\u0026quot;, level,\u0026quot;.csv\u0026quot;)\rorderbook_filename \u0026lt;- paste0(asset, \u0026quot;_\u0026quot;,date,\u0026quot;_34200000_57600000_orderbook_\u0026quot;, level,\u0026quot;.csv\u0026quot;)\rLet’s have a look at the raw message feed first.\nmessages_raw \u0026lt;- read_csv(messages_filename, col_names = c(\u0026quot;ts\u0026quot;, \u0026quot;type\u0026quot;, \u0026quot;order_id\u0026quot;, \u0026quot;m_size\u0026quot;, \u0026quot;m_price\u0026quot;, \u0026quot;direction\u0026quot;, \u0026quot;null\u0026quot;),\rcol_types = cols(ts = col_double(), type = col_integer(),\rorder_id = col_integer(),\rm_size = col_double(),\rm_price = col_double(),\rdirection = col_integer(),\rnull = col_skip())) %\u0026gt;% mutate(ts = as.POSIXct(ts, origin=date, tz=\u0026quot;GMT\u0026quot;), m_price = m_price / 10000)\rmessages_raw\r## # A tibble: 20,000 x 6\r## ts type order_id m_size m_price direction\r## \u0026lt;dttm\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt;\r## 1 2020-03-17 09:30:00 4 24654260 230 245 1\r## 2 2020-03-17 09:30:00 3 24683304 500 245. -1\r## 3 2020-03-17 09:30:00 3 24690848 500 245. 1\r## 4 2020-03-17 09:30:00 1 24699256 500 245. -1\r## 5 2020-03-17 09:30:00 3 24690812 500 245. -1\r## 6 2020-03-17 09:30:00 3 24699256 500 245. -1\r## 7 2020-03-17 09:30:00 1 24699992 500 245. 1\r## 8 2020-03-17 09:30:00 1 24700384 500 245. 1\r## 9 2020-03-17 09:30:00 3 24700384 500 245. 1\r## 10 2020-03-17 09:30:00 1 24700516 500 245. 1\r## # ... with 19,990 more rows\rBy default, ts denotes the time in seconds since midnight (decimals are precise until nanosecond level) and price always comes in 10.000 USD. type denotes the message type: 4, for instance, corresponds to the execution of a visible order. The remaining variables are explained in more detail here.\nNext, the corresponding orderbook snapshots contain price and quoted size for each of the 10 levels.\norderbook_raw \u0026lt;- read_csv(orderbook_filename,\rcol_names = paste(rep(c(\u0026quot;ask_price\u0026quot;, \u0026quot;ask_size\u0026quot;, \u0026quot;bid_price\u0026quot;, \u0026quot;bid_size\u0026quot;), level),\rrep(1:level, each=4), sep=\u0026quot;_\u0026quot;),\rcols(.default = col_double())) %\u0026gt;% mutate_at(vars(contains(\u0026quot;price\u0026quot;)), ~./10000)\r\rPutting the files together\rEach message is associated with the corresponding orderbook snapshot at that point in time.\rAfter merging message and orderbook files, the entire data thus looks as follows\norderbook \u0026lt;- bind_cols(messages_raw, orderbook_raw) \r\r\rts\rtype\rorder_id\rm_size\rm_price\rask_price_1\rask_size_1\rbid_price_1\rbid_size_1\r\r\r\r2020-03-17 09:30:00\r4\r24654260\r230\r245.00\r245.10\r500\r244.88\r1000\r\r2020-03-17 09:30:00\r3\r24683304\r500\r245.14\r245.10\r500\r244.88\r1000\r\r2020-03-17 09:30:00\r3\r24690848\r500\r244.88\r245.10\r500\r244.88\r500\r\r2020-03-17 09:30:00\r1\r24699256\r500\r245.03\r245.03\r500\r244.88\r500\r\r2020-03-17 09:30:00\r3\r24690812\r500\r245.10\r245.03\r500\r244.88\r500\r\r2020-03-17 09:30:00\r3\r24699256\r500\r245.03\r245.11\r500\r244.88\r500\r\r\r\r\r\rCompute summary statistics\rNext, I compute summary statistics on 20 second levels. In particular I am interested in quoted prices, spreads, and depth (the amount of tradeable units in the orderbook):\n\rMidquote \\(q_t = (a_t + b_t)/2\\) (where \\(a_t\\) and \\(b_t\\) denote the best bid and best ask)\rSpread \\(S_t= (a_t - b_t)\\) (values below are computed in basis points relative to the concurrent midquote)\rVolume is the aggretate sum of traded units of the stock. I do differentiate between hidden (type==5) and visible volume.\r\rorderbook \u0026lt;- orderbook %\u0026gt;% mutate(midquote = ask_price_1/2 + bid_price_1/2, spread = (ask_price_1 - bid_price_1)/midquote * 10000,\rvolume = if_else(type ==4|type ==5, m_size, 0),\rhidden_volume = if_else(type ==5, m_size, 0))\rAs a last step, depth of the orderbook denotes the number of assets that can be traded without moving the quoted price more than a given range (measured in basis points) from the concurrent midquote. The function below takes care of the slightly involved computations.\ncompute_depth \u0026lt;- function(df, side = \u0026quot;bid\u0026quot;, bp = 0){\rif(side ==\u0026quot;bid\u0026quot;){\rvalue_bid \u0026lt;- (1-bp/10000)*df %\u0026gt;% select(\u0026quot;bid_price_1\u0026quot;) index_bid \u0026lt;- df %\u0026gt;% select(contains(\u0026quot;bid_price\u0026quot;)) %\u0026gt;% mutate_all(function(x) {x \u0026gt;= value_bid})\rsum_vector \u0026lt;- (df %\u0026gt;% select(contains(\u0026quot;bid_size\u0026quot;))*index_bid) %\u0026gt;% rowSums()\r}else{\rvalue_ask \u0026lt;- (1+bp/10000)*df %\u0026gt;% select(\u0026quot;ask_price_1\u0026quot;)\rindex_ask \u0026lt;- df %\u0026gt;% select(contains(\u0026quot;ask_price\u0026quot;)) %\u0026gt;% mutate_all(function(x) {x \u0026lt;= value_ask})\rsum_vector \u0026lt;- (df %\u0026gt;% select(contains(\u0026quot;ask_size\u0026quot;))*index_ask) %\u0026gt;% rowSums()\r}\rreturn(sum_vector)\r}\rorderbook \u0026lt;- orderbook %\u0026gt;% mutate(depth_bid = compute_depth(orderbook),\rdepth_ask = compute_depth(orderbook, side=\u0026quot;ask\u0026quot;),\rdepth_bid_5 = compute_depth(orderbook, bp = 5),\rdepth_ask_5 = compute_depth(orderbook, bp = 5, side=\u0026quot;ask\u0026quot;))\rAlmost there! The snippet below splits the data into 20 second intervals and computes the averages of the computed summary statistics.\norderbook_dense \u0026lt;- orderbook %\u0026gt;%\rmutate(ts_minute = floor_date(ts, \u0026quot;20 seconds\u0026quot;)) %\u0026gt;% select(midquote:ts_minute) %\u0026gt;% group_by(ts_minute) %\u0026gt;% mutate(messages = n(),\rvolume = sum(volume),\rhidden_volume = sum(hidden_volume)) %\u0026gt;%\rsummarise_all(mean)\rHere we go: during the first 100 seconds on March 17th, 20.000 messages related to the orderbook of SPY have been processed by NASDAQ. The quoted spread on average was around 3bp. On average, roughly 90.000 contracts have been traded during each 20 second slot - in other words, assets worth roughly 90 million USD have been exchanged. Quoted liquidity at the best bid and best ask seems rather small relative to the tremendous amounts of trading activity during this (very short) period of time.\n\r\rts_minute\rmidquote\rspread\rvolume\rhidden_volume\rdepth_bid\rdepth_ask\rdepth_bid_5\rdepth_ask_5\rmessages\r\r\r\r2020-03-17 09:30:00\r245.0332\r4.010257\r89606\r19923\r353.7358\r354.1362\r1854.152\r2516.916\r5890\r\r2020-03-17 09:30:20\r245.2229\r3.142070\r54733\r23716\r190.3232\r238.8164\r2099.857\r2041.646\r3165\r\r2020-03-17 09:30:40\r245.5052\r2.177630\r53273\r18188\r121.9574\r182.5553\r2113.945\r2282.149\r4246\r\r2020-03-17 09:31:00\r245.2010\r1.488751\r146974\r86780\r297.4000\r254.3316\r1985.406\r2416.603\r4210\r\r2020-03-17 09:31:20\r244.6590\r1.514445\r26286\r6655\r122.6870\r115.6107\r2174.080\r2325.517\r2489\r\r\r\rFinally, some visualisation of the data at hand: The code below creates the figure at the beginning of the post and shows the dynamics of the traded prices (red line) and the quoted prices at the higher levels of the orderbook.\norderbook_trades \u0026lt;- orderbook %\u0026gt;% filter(type==4|type==5) %\u0026gt;% select(ts, m_price)\rorderbook_quotes \u0026lt;- orderbook %\u0026gt;% mutate(id = row_number()) %\u0026gt;%\rselect(ts, id, matches(\u0026quot;bid|ask\u0026quot;)) %\u0026gt;% gather(level, price, -ts, -id) %\u0026gt;%\rseparate(level, into=c(\u0026quot;side\u0026quot;,\u0026quot;variable\u0026quot;,\u0026quot;level\u0026quot;), sep=\u0026quot;_\u0026quot;) %\u0026gt;%\rmutate(level = as.numeric(level)) %\u0026gt;% spread(variable, price)\rp1 \u0026lt;- ggplot() + theme_bw() +\rgeom_point(data = orderbook_quotes, aes(x=ts, y=price, color=level, size = size/max(size)), alpha = 0.1)+\rgeom_line(data = orderbook_trades, aes(x=ts, y=m_price), color=\u0026#39;red\u0026#39;) + labs(title=\u0026quot;SPY: Orderbook Dynamics\u0026quot;,\ry=\u0026quot;Price\u0026quot;,\rx=\u0026quot;\u0026quot;) +\rtheme(panel.grid.major = element_blank(),\rpanel.grid.minor = element_blank(),\rlegend.position =\u0026quot;none\u0026quot;) +\rscale_y_continuous()\r\r","date":1585094400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585297296,"objectID":"2a759b70efd04c772da2747c308e2260","permalink":"https://voigtstefan.me/post/lobster-1/","publishdate":"2020-03-25T00:00:00Z","relpermalink":"/post/lobster-1/","section":"post","summary":"A short  series of posts on handling high-frequency data from Lobster and R","tags":[],"title":"LobsteR - NASDAQ under a \"tidy\" Microscope","type":"post"},{"authors":["Nikolaus Hautsch","Christoph Scheuch","Stefan Voigt"],"categories":[],"content":"This paper replaces an earlier draft titled \u0026ldquo;Limits to Arbitrage in Markets with Stochastic Settlement Latency\u0026rdquo;.\n","date":1584662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584662400,"objectID":"918fffb669ad3e0ef949c3db8ebe0735","permalink":"https://voigtstefan.me/publication/trust-takes-time-limits-to-arbitrage-in-blockchain-based-markets/","publishdate":"2020-03-19T15:38:53+01:00","relpermalink":"/publication/trust-takes-time-limits-to-arbitrage-in-blockchain-based-markets/","section":"publication","summary":"Distributed ledger technologies replace trusted clearing counterparties and security depositories with time-consuming consensus protocols to record the transfer of ownership. This settlement latency exposes cross-market arbitrageurs to price risk. We theoretically derive arbitrage bounds that increase with expected latency, latency uncertainty, volatility and risk aversion. Using Bitcoin orderbook and network data, we estimate arbitrage bounds of on average 121 basis points, explaining 91% of the observed cross-market price differences. Consistent with our theory, periods of high latency-implied price risk exhibit large price differences, while asset flows chase arbitrage opportunities. Blockchain-based settlement thus introduces a non-trivial friction that impedes arbitrage activity. ","tags":[],"title":"Building Trust Takes Time: Limits to Arbitrage in Blockchain-Based Markets","type":"publication"},{"authors":["Stefan Voigt"],"categories":[],"content":"","date":1584576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584576000,"objectID":"f70f2bc58c28548ce81c9907b3bcd93f","permalink":"https://voigtstefan.me/publication/liquidity-and-price-informativeness-in-blockchain-based-markets/","publishdate":"2019-03-20T15:43:57+01:00","relpermalink":"/publication/liquidity-and-price-informativeness-in-blockchain-based-markets/","section":"publication","summary":"Costs on cross-market trading impose limits to arbitrage and harm price informativeness in fragmented markets. I quantify the impact of the time-consuming settlement process in the market for Bitcoin on arbitrageurs activity. The estimation rests on a novel threshold error correction model that exploits the notion that arbitrageurs suspend trading activity when arbitrage costs exceed price differences. I estimate substantial arbitrage costs that explain 63\\% of the observed price differences, where more than 75\\% of these costs can be attributed to settlement latency. I also find that a 10~bp decrease in latency-related arbitrage costs simultaneously results in a 3~bp increase of the quoted bid-ask spreads. I reconcile this finding in a theoretical model in which liquidity providers set larger spreads to cope with the higher adverse selection risks imposed by increased arbitrage activity. Consequently, efforts to reduce the latency of blockchain-based settlement might have unintended consequences for liquidity provision. In markets with substantial adverse selection risk, faster settlement may thus even harm price informativeness.","tags":[],"title":"Liquidity and Price Informativeness in Blockchain-Based Markets","type":"publication"},{"authors":["Stefan Voigt","Nikolaus Hautsch"],"categories":[],"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"b45959558b23aae2294c2bbc2a8b1db9","permalink":"https://voigtstefan.me/publication/large-scale-portfolio-optimization-under-transaction-costs-and-model-uncertainty/","publishdate":"2019-04-11T00:00:00Z","relpermalink":"/publication/large-scale-portfolio-optimization-under-transaction-costs-and-model-uncertainty/","section":"publication","summary":"We theoretically and empirically study portfolio optimization under transaction costs and establish a link between turnover penalization and covariance shrinkage with the penalization governed by transaction costs. We show how the ex ante incorporation of transaction costs shifts optimal portfolios towards regularized versions of efficient allocations. The regulatory effect of transaction costs is studied in an econometric setting incorporating parameter uncertainty and optimally combining predictive distributions resulting from high-frequency and low-frequency data. In an extensive empirical study, we illustrate that turnover penalization is more effective than commonly employed shrinkage methods and is crucial in order to construct empirically well-performing portfolios.","tags":[],"title":"Large Scale Portfolio Optimization under Transaction Costs and Model Uncertainty","type":"publication"}]